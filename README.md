# Temporal Action Recongition with MVD

The Supervisely Team is pleased to share one more successfull solution of solving the task of Temporal Action Recognition using the [Masked Video Distillation](https://github.com/ruiwang2021/mvd) model.

## Description

The initial goal of the project is to provide an effective solution that can be used to automate the process of monitoring the changes in the behavior of mouse before and after an experimental treatment. To achive this goal, two main actions were identified: "Head/Body Twitch" and "Self-Grooming". The task is to detect and calculate the duration and frequency of these actions in the videos. By comparing the results of the analysis before and after the treatment, it is possible to determine whether the treatment has had a positive or negative effect on the behavior of the mouse.

>>> Вставить ссылку на публикацию.

**Data type:** Video  
**Task types:** Temporal Action Recognition, Object Detection  
**Used models:** MVD, RT-DETRv2  
**Pre-processing:** Video frames extraction and sampling, object detection and cropping the frames, trimming the video into segments

## Solution Approach

The Supervisely Team focused on findint the optimal solution that can be used to solve this case in the most effective way. The solution is based on the following steps:

1. **Upload the video files** into the Supervisely platform. For already annotated videos, the initial annotations were made outside of Supervisely in a specific custom format, so the Supervisely Team developed a [custom script](https://github.com/supervisely-ecosystem/mouse-tests) that can be used to convert and import those annotations.
2. **Perform video frames extraction and sampling** using the [custom script](https://github.com/supervisely-ecosystem/script-example/blob/sampling_video_project/src/main.py), written using Supervisely Python SDK. In this case, we decided to extract frames from different segments of the video, splitting the frame ranges into quarters that represent the different parts of the video. This allows to cover the whole video and not to miss any important action that can be performed by the mouse.
3. **Use pre-trained RT-DETRv2 model** to detect the mouse in the video frames. We used the class "cat" to detect the mouse and it performed well enough to detect the mouse in the video frames.
4. **Filter out the frames** that do not contain the mouse using the [Filter Images](https://ecosystem.supervisely.com/apps/filter-images) application from Supervisely Ecosystem and prepare the training dataset for the Object Detection model.
5. **Use the [Quality Assurance & Statistics](https://docs.supervisely.com/data-organization/quality-assurance-and-statistics)** features of Supervisely platform to view, check and analyze the training dataset, that will be used to train the Object Detection model.
6. **Train the Object Detection model** on the prepared dataset using the [Train RT-DETRv2](https://ecosystem.supervisely.com/apps/rt-detrv2/supervisely_integration/train) application.
7. **Check the benchmark report** of the trained model that was automatically generated by the [Evaluator for Model Benchmark](https://ecosystem.supervisely.com/apps/model-benchmark) application after the training process is finished. Analyze the results and check the quality of the model.
8. **Use the trained Object Detection model** to detect the mouse in the video frames and crop the frames to the bounding boxes of the mouse.
9. **Trim videos into segments** of ~2-3 seconds each, which are manageable for the MVD model. Each segment represents a particular mouse action.
10. **Use those segments as training data** for the MVD model, which is used to recognize the actions of the mouse in the video segments.

![Solution Approach](/assets/solution-approach.png)

## Solution Overview

The complete worfkflow of the solution contains the several steps, each of them is described in the corresponding section below. The solution is implemented using the Supervisely platform and its features, such as video frames extraction, object detection, training models, and analyzing the results.

### Import data

The first step of the solving the task is to import the video files into the Supervisely platform. There are plenty of ways to do that, and all of them described in the [Import](https://docs.supervisely.com/import-and-export/import) section of the Supervisely documentation. In this bcase we'll briefly describe one of the options - manual upload of the data from the local machine.

1. Create a new project in Supervisely, name it Input Project. If your workspace is empty, you can just click the Import Data button.
2. Choose the Videos option in the Type of project section, and click Create.
3. Next, you can drag and drop your video files into the project.

If you need to import files from a remote server or from a Cloud Storage, you can use apps like [Import Videos from Cloud Storage](https://ecosystem.supervisely.com/apps/import-videos-from-cloud-storage) or [Remote Import](https://ecosystem.supervisely.com/apps/remote-import).

For this case, the annotations were made in a specific custom format outside of Supervisely platform, so the Supervisely Team developed a [custom script](https://github.com/supervisely-ecosystem/mouse-tests) that can be used to convert and import those annotations into Supervisely platform. The script is written using Supervisely Python SDK and it's possible to implement any custom logic for various custom formats and even wrap this script into a [Supervisely App](https://developer.supervisely.com/app-development/basics/from-script-to-supervisely-app) for more convenient usage.

### Annotate video files

Before training a NN model, you need to annotate your Input Project. We recommend to annotate your videos in Supervisely, because this way you can annotate precise frames where an action starts and ends, which will help the model learn much better. Also, the labeling tool is very convenient and has a lot of features that will help you annotate your data faster.

Annotation format: The annotations should contain the mouse actions (Head/Body Twitch, Self-Grooming) represented as Tags. Each tag has a start time and end time, where the action is happening. You don't need to manually annotate bounding boxes around the mouse, because our mouse detector will do it at the preprocessing stage.

![Annotate video files](/assets/annotating.png)

1. Import the video files into Supervisely platform.
2. Perform video frames extraction and sampling using the [custom script](https://github.com/supervisely-ecosystem/script-example/blob/sampling_video_project/src/main.py).

1. Import video files into Supervisely platform along with the corresponding initial annotations, that were made outside of Supervisely using the [custom script](https://github.com/supervisely-ecosystem/mouse-tests) that the Supervisely Team developed for the specific annotation format.  
2. Annotate the video files with the frame-based Tags that indicate the start and end of the action corresponding the the specified action type.
3. 
3. Using the RT-DETRv2 model, detect the mouse in the video frames and crop the frames to the bounding boxes of the mouse.